---
title: "Data management and archival with APECS"
author: "02 Nov 2018; Tiff Stephens"
output: pdf_document
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(knitr)
```

---

## **1. Introduction**

Great work in collecting mountains of data! The next step is to make sure that we are all on the same page with how these data (and metadata) are handled so that they are accessible to other APECS members, as well prepped appropriately for long-term storage via archival. This document provides a guideline for how APECS preps data files for storage and analysis to help ensure that we meet the goals of our management plan. Each of you will help APECS reach such goals via curating the data that you collected and working with Tiff to eventually archive it.

\vskip 0.1in

First, you should be aware that both grants under the APECS umbrella have a data management plan. They are not complex but serve as a promise to ourselves and to NSF that we will be responsible with the expensive data that we were funded to collect. The plans outline:

* That we will be compliant with NSF data management and dissemination policies (essentially: don’t lose data, keep it clean, and make it available to others). 
* That all data is secure during the project (e.g. backed-up).
* That all datasets will be accompanied by appropriate metadata.
* That all data will be submitted to appropriate data repositories no later than two years after being collected. Eventually all should be publicly available.
* That we will submit data in an accessible and flexible format (i.e. .csv files).

\vskip 0.2in

**A summary of steps required to meet our data management goals**

* Make sure that entered data are accurate and that they the dataset is in a "clean" format.
* Have a Masterfile for each dataset that will not be manipulated.
* Make sure that each dataset has appropriate metadata. 
* Store datasets in locations accessible to all APECS members (i.e. shared Google Drive and GitHub)
* Work with Tiff to archive data with selected repositories (i.e. KNB and BCO-DMO)

\vskip 0.5in




---

## **2. Enter raw data and turn it into a clean and defined dataset**

##### Enter data

After fieldwork, labwork, etc., raw data need to be collated into a spreadsheet. To save time, be mindful of how those data are organized in a working dataset before entering data. For example, you don't want to have to go through and copy + paste things into different columns later (*note: regardless of time, this isn't recommended because it often results in human-induced errors within the dataset*). Tiff likes to enter everything into as few of columns as possible (e.g. multiple levels included in one column/factor) and then use R functions to manipulate the structure of the dataframe later. Datasets should be "clean", which, convenient for some, aligns with structure appropriate for analysis in R.

\vskip 0.1in

*What is a clean dataset?* Essentially, each variable (dependent and independent) should have its own column, where the rows in each column includes a single point of information. Columns can have multiple levels included to help characterize single datapoints across rows -- for example a "sea_otter_region" column can have "high", "mid", and "low" factors within them.

\vskip 0.2in

**Some fundamental guidelines for structuring a dataset are described below, paired with Fig 1:**

**A.** Text in column titles and in rows: Some statistical packages do not like spaces or slashes in column titles. Instead use an underscore or period between words/characters. Typically, spaces in rows below each column title is just fine. 

\vskip 0.05in

**B.** It's easiest, and more universal, to keep latitudes and longitudes in decimal degrees. Make sure that this is defined in the column title and/or the metadata. 

\vskip 0.05in

**C.** For dates, please annotate what format your data are in, especially in the metadata but it is also useful for column titles. e.g. "date_MM.DD.YY", "date_YYYY.MM.DD". Date data are notorious troublemakers in both analysis and archival, especially for international access. 

\vskip 0.05in

**D.** These columns are examples of how multiple levels can be included in one factor (column). Notice how the information is nested, i.e. that each replicate is nested within its respective transect, nested within the respective site. Depending on your preference, these different levels can occupy separate columns (like shown with the density data in columns K and L).

\vskip 0.05in

**E.** Note that if there are site-level data, such needs to be filled in for EVERY unique datapoint.

\vskip 0.05in

**F.** Two issues here: (1) running calculations in Excel and (2) saving files with calculation sections or seemingly random boxes of data/information (see bottommost rows). Unless you're running data analysis in Excel (please don't do this), there should be no calculations saved in the spreadsheet. If you're using R for analysis, please avoid calculating *anything* in Excel, even simple sums or means...mistakes will happen. In this example, it is better to leave the two density columns as raw and calculate their means using code in R because it is reproducible and more reliable. The second issue will be misinterpretted by your statistical software; whatever platform you use incorporates information from the entire column called upon and thus would include "Averages", "Nossuk", "N Pass", and "Shinaku" as levels in the sediment columns, and the associated mean values as new datapoints in the pits column. Just don't do these things.

\vskip 0.1in

```{r, out.width = "96%", fig.align = "left", echo = FALSE}
knitr::include_graphics(path = "/Users/tiff/Desktop/R Studio/APECS-master-repos/data_management_plan/images/datasheet.png")
```
**Figure 1** Example dataset used as an example for the guidelines listed above.

\vskip 0.5in

##### Don't forget to QC/QA.

It's necessary to check ALL of the entered data with assure that it is accurate; hopefully it goes faster than the initial entry. Even after verifying everything, some like to review the dataset in R to see if there are any odd entries or values that don’t make sense before fully accepting that the QC/QA process is complete. There are *a lot* of different ways to check data (many opinions and strategies outlined on the web) but it is good practice to (1) at least determine that functions like "summary()" work without incurring errors and (2) manually sort each column in the dataframe to check for obvious errors (e.g. odd placement of "NA" or accidental character inclusion, like an "." instead of "0" or "NA"). 

\vskip 0.2in

##### Metadata!

It is absolutely critical to build descriptive metadata for each dataset. What do I mean by metadata?:

\vskip 0.1in

+ **Short, direct definitions**: This often involves transposing the column titles used in the dataset and providing an informative definition of each factor in your datasheet (example below). This can be saved as a seperate tab in an Excel document but will ultimately need to be a separate .csv file to pair with your data for archival.

    | Factor          | Description                                                                     
    |-----------------|-------------------------------------------------------------------------
    | site_name       | Identifying name of the field site                                              
    | longitude       | Longitude (E) of the site in decimal degrees                                    
    | latitude        | Latitude (N) of the site in decimal degrees                                     
    | date_MM.DD.YY   | Date of sampling expressed as month, day, and year (MM/DD/YY)                   
    | so_region       | Regional categorization of sea otter presence; 3 levels: high, mid, low   
    | transect        | Transect location within each site relative to the seagrass bed                 
    | sed_primary     | Qualitative characterization of primary sediment type (e.g. sand, mud)      
    | n_pits          | Number of pits counted within the transect replicate                            
    | n_otters        | Number of sea otters counted in the boat survey                         

\vskip 0.2in

+ **Brief summary**: This should include information about the purpose of the dataset and basic summary of where and how it was collected. Think "briefer version of the methods section". This is an extremely valuable description for people that will access the publicly available data years down the road, it can be the clencher for whether your data are usable/interpretable by future parties. This is a requirement for submitting to data repositories, a real-life KNB example shown below: 


```{r, out.width = "90%", fig.align = "center", echo = FALSE}
knitr::include_graphics(path = "/Users/tiff/Desktop/R Studio/APECS-master-repos/data_management_plan/images/metadata.png")
```

\vskip 0.5in



## **3. Day-to-day storage of data**

The two most important things to do remember when storing data are (1) to not lose the data and (2) to keep a **Masterfile** of each datasheet (after QC/QA) that will not be manipulated. There are two places that APECS stores data so that it is safe and accessible by APECS members: the shared Google Drive and GitHub platforms that APECS curates. Backing-up data on personal/lab hard drives and computers is also approprate (if not inevitable). Keep in mind, however, that it is important to be weary and conscious of multiple file versions. In the end, APECS *really* only wants two versions of each datasheet for APECS-related storage and analysis -- ** RAW and CLEAN versions** -- storing data and knowing which version to use for analysis can get messy if you have 20 different versions of the same file. 

\vskip 0.2in

+ **Google Drive**: I think that we're all familiar with this, already, since many have their own folder within the data folder in the shared drive. If you don't have one, please start one, and if you don't have access, please ask me for it. It's fine to leave these files as google files (as opposed to .csv), but it is important to have obvious names for each file, noting whether it is raw or clean. 

```{r, out.width = "75%", fig.align = "center", echo = FALSE}
#knitr::include_graphics(path = "/Users/tiff/Desktop/R Studio/APECS-master-repos/data_management_plan/images/datadrive1.png")
knitr::include_graphics(path = "/Users/tiff/Desktop/R Studio/APECS-master-repos/data_management_plan/images/datadrive2.png")
```

\vskip 0.2in

+ **GitHub**: Tiff and Wendel have been working from the APECS GitHub folder, meaning that in addition to storage in the Google Drive, we store datasets in the "APECS-master-repos" folder to call upon in RStudio (or R) for data manipulation and analysis. We held a workshop on this last year but since few people had datasets to practice with, it is 100% understandable and recomended to consult with either Tiff or Wendel if you want to get in on the GitHub stoke. As a reminder, you'll want to have a local GitHub folder system on your local computer that links to the online GitHub repository. 

```{r, out.width = "75%", fig.align = "center", echo = FALSE}
knitr::include_graphics(path = "/Users/tiff/Desktop/R Studio/APECS-master-repos/data_management_plan/images/githubfolders2.png")
```

\vskip 0.5in




## **4. Long-term archival of data**

Generally, you will work with Tiff to successfully upload all APECS-collected data into preferred data repositories. In APECS' data management plans with NSF, we committed to archiving all data with BCO-DMO ([Biological and Chemical Oceanography Data Management Office](https://www.bco-dmo.org/)) but are also submitting to KNB ([Knowledge Network for Biocomplexity](https://knb.ecoinformatics.org/)). The latter is more user friendly and the archived data is accessible to BCO-DMO. 

\vskip 0.1in

The important prep information for data archival was largely covered in the section about data structure and metadata -- i.e. your data need to be in shape so that anyone can interpret and use them. The archival process involve the following files:

+ Dataset that is cleaned (required)
+ Metadata (required)
+ Raw dataset (recommended but optional; great for reproducibility)
+ Code used to transform raw data into working data (recommended but optional; great for reproducibility)
+ Diagram to help convey methods for data collection (optional)

\vskip 0.2in

There isn't much more to say here other than archival is a serious business and it's best to be thorough because you don't know who will access your dataset, what they'll use it for, and whether you'll still be alive if they have questions. You can look at an example from last year, where Tiff archived 2017 data using a series of submissions to KNB (https://knb.ecoinformatics.org/view/knb.92121.19). *Note: KNB has been updating their user platform a lot, recently, so even Tiff needs to give it a new browse to make sure that functionality is as she knows it.*




